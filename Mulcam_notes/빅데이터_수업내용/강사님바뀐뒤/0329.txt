03.29


하둡이 시작될 때 네임노드가 에디트로그를 읽고 그걸로 데이터 테이블을 만든다.
이런식으로 운영되는 것이 로그파일시스템, 그러면 로그가 너무 많이 쌓이게되는데 그걸 일부분씩 쪼개서 객체화 시키면 그것이 저널
그리고 그런 방식으로 저널들을 읽고 남은 조금의 로그를 읽어서 만드는게 저널파일시스템

하둡 파일시스템 쓰기
먼저 systemctl stop firewalld로 방화벽 끄기
su hadoop들어가기
start-all.sh
[hadoop@s1 ~]$ hdfs dfs -ls / 
(hdfs dfs) 라고 쓰는건 관행
하둡에 있는 파일 복사
hdfs dfs -copyFromLocal hello.txt /

느낌적인 느낌으로 안보고는 쓰지

1. 수집
웹에서 수집 (안주면 크롤링, 주면 OpenAPI) 
내부시스템에서 축적되는 데이터
파이썬(리눅스에서 자바보다 좀 더 가벼움)에서 리눅스 명령어로 수집, 가공 저장
파이썬을 활용한 데이터 크롤링

2. 저장
HDFS
하둡에 저장

3. 처리
MapReducer
자바를 이용해 다양한 맵리듀스를 필요에 맞게 만들 수 있다,
PIG와 HIVE를 이용해 처리한다.
wordcount 맵 리듀스 작업 돌리고

4. 분석
R, Spark, Hive
정렬, 상관, 연관, 회귀, 분류, 군집 등으로 분석함
R분석 활용을 배우고 R자바를 연동시킨다.

5. 시각화
d3.js
billboad.js 등으로 프론트단에서 직접 그리거나 R에서 시각화
차트로 표현

[책으로 실습]
먼저 구텐베르크 프로젝트에서 책 한권을 wget으로 받아온다.(plain utf-8 버전)
[hadoop@s1 ~]$ wget http://www.gutenberg.org/files/56850/56850-0.txt

다음으로 받은 책을 하둡파일시스템에 넣음
hdfs dfs -copyFromLocal 56850-0.txt /ebook

해당 폴더로 이동
cd /data/hadoop/hadoop-2.8.2/share/hadoop/mapreduce

하둡에 어떤 기능이 있나 확인
hadoop jar hadoop-mapreduce-examples-2.8.2.jar

다음과 같은 방식으로 처리
[hadoop@s1 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.8.2.jar wordcount /ebook /ebook_out
앞에꺼는 입력, 뒤에꺼는 출력 결과가 들어갈 폴더 명
주의. 출력 폴더는 존재하지 않은 폴더여야한다. 이미 동명의 폴더가 있으면 작동안함

결과확인
[hadoop@s1 ~]$ hdfs dfs -ls /ebook_out
18/03/29 22:07:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   3 hadoop supergroup          0 2018-03-29 22:04 /ebook_out/_SUCCESS
-rw-r--r--   3 hadoop supergroup      57583 2018-03-29 22:04 /ebook_out/part-r-00000

결과안에 내용 보기
hdfs dfs -cat /ebook_out/part-r-00000

결과 정렬하기
hdfs dfs -cat /ebook_out/part-r-00000 | awk '{print $2", " $1}' | sort -nr | more
[hadoop@s1 ~]$ hdfs dfs -cat /ebook_out/part-r-00000 | awk '{print $2", " $1}' | sort -nr | more

결과를 csv 파일에 담기
[hadoop@s1 ~]$ hdfs dfs -cat /ebook_out/part-r-00000 | awk '{print $2", " $1}' | sort -nr | awk -F , '{print $2", "$1}' > w.csv

