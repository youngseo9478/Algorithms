0416
하나씩 연결되어있으면 리스트
두개씩 나와있으면 바이너리트리
막 서로 연결되어있으면 그래프

스파크는 오만게 다 가능하다.
스파크는 맵리듀스를 쓰지 않고도 자원관리를 통해 연산을 한다. 이걸 위해서 하둡 2.0과 yarn에 나왔다고 해도 괜찮음.
다양한 언어를 지원
자바 스칼라 파이선 R sql을 지원하고 스칼라가 메인언어 
스칼라나 파이썬 같은 함수형 프로그래밍언어는 분산작업을 처리하는 코드를 짜기에 유리

스파크느느
대용량 분산 데이터 처리/분석에서 종합선물세트
분석 라이브러리가 많고 하나하나 내가 다 짜야할 필요가 없다.

스파크는 데이터가 처리되고 또 다른 작업이 처리되고 하는 방식을 파일을 만들지 않고 메모리로 한다.
하둡에서는 매번 작업마다 파일로 쓰는 과정이 있었다. 그래서 스파크는 중간에 터지면 처음부터 함
어떤 작업을 어떤 순서로 할 지 계보(리니지)로 실행계획만 가지고 있고
실제로 연산이 시작되는 것은 액션이 있은 후이다. 
중간에 값이 변한다면 새로운 계보를 만들어낸다.

스파크는 RDD라는 데이터 타입을 기본으로 사용하고 RDD는 변하지 않으며 생성을 위한 계보를 가지고 있어서 언제든 다시 만들 수 있다.
연산을 할 때마다 새로운 RDD가 생성된다. 그래서 디스크에 다 기록하지 않아도 살아있게 된다.
새로운 변화가 생겼을 때 기존의 파일을 변경하지 않고 새로운 RDD를 만들어낸다.

===================================================스파크 설치=======================================================
[hadoop@s1 ~]$ wget http://mirror.apache-kr.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
이클립스에서 scala 검색 후 scala IDE 를 설치

그다음 파일을 /data/hadoop/에 압축을 풉니다.
[hadoop@s1 ~]$ tar -xvf spark-2.3.0-bin-hadoop2.7.tgz
[hadoop@s1 ~]$ mv spark-2.3.0-bin-hadoop2.7 /data/hadoop/
이동 시킴 전후에 ls 해보면 좋음

환경변수에 등록
[hadoop@s1 ~]$ vi .bashrc
#For Spark setting
export SPARK_HOME=/data/hadoop/spark-2.3.0-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
적고 
:wq

[hadoop@s1 ~]$ source .bashrc
로 재실행 해주고

[hadoop@s1 ~]$ spark 까지 적은 상태에서 tab 해주면
spark-class   sparkR        spark-shell   spark-sql     spark-submit
이 결과가 나오면 제대로 설치가 끝난 것임.

스파크가 제공하는 3가지 클러스터 모드
1.standalone은 혼자 동작 (스스로 클러스터를 구동) start-all.sh로 구동
2.yarn에 붙어서 동작 yarn으로부터 관리 (yarn모드는 task를 받아서 노드매니저가 task를 수행)
3.mesos에 붙어서 동작 mesos로부터 클러스터 관리 (Mesos는 분산환경에서 작업을 최적화)

일반적으로 처리 분석할 데이터가 hdfs에 있으면 yarn이나 Mesos가 더 유리 (우리는 yarn으로 간다)

SparkContext를 가지고 있는 시작점이 드라이버고 이게 스파크 어플리케이션의 중심이다.
그리고 각 워커에서 task를 던져주고 일을 시킴


스파크 해보기
[hadoop@s1 ~]$ spark-shell
2018-04-16 09:54:59 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://s1:4040
Spark context available as 'sc' (master = local[*], app id = local-1523841914188).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.0
      /_/

Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_161)
Type in expressions to have them evaluated.
Type :help for more information.
(JDK가 없으면 안돌아가는 JDK기반 프로그램이다)
(master가 local[*]로 설정되어 있는데 local은 1대로 돌린다는 것이고 [*]은 사용할 쓰레드의 숫자로 *은 쓸 수 있는 만큼
그리고 master는 yarn, Mesos, standalone이 가능하다/.

scala> 1+1
res0: Int = 2
(변수 이름을 주지 않으면 자동으로 생성)
scala> val num = 1+2
num: Int = 3
(val은 한번 데이터에 가서 붙으면 떨어지지 않는 변수, 내용을 바꾸지 못하는 final느낌)
scala> var number = 2
number: Int = 2

scala> number = 3
number: Int = 3
(변수의 내용을 바꿀 수 있는 변수를 만들고 싶으면 var를 쓰면 된다.)

def 함수명
(Unit는 void느낌)
scala> def sayHello(): Unit = print("hello")
sayHello: ()Unit

scala> def sayHello2(): Unit = {
     | print("hello")
     | println("Hello")
     | println("Welcome")
     | }
sayHello2: ()Unit

scala> def addOne(m:Int):Int = m+1
addOne: (m: Int)Int

scala> addOne(9)
res4: Int = 10
(매개변수를 넣어서 만든 함수, 매개변수 타입을 일일이 지정해줘야하고 또한 ):Int는 리턴타입)

scala> def adder(m:Int,n:Int) = m+n
adder: (m: Int, n: Int)Int

scala> val add2 = adder(2,_:Int)
add2: Int => Int = <function1>

scala> add2(6)
res5: Int = 8

(되다가 만 함수를 만들 수 있다. 다음번에 빈 매개변수 하나만 넣어주면 완성되어서 값을 배출함)

scala> (x:Int) => x+1
res6: Int => Int = <function1>
(이름이 없어도 자동생성이름을 가진 함수가 되고 이는 res6(8)과 같은 식으로 실행이 가능합니다.)
scala> def MyFunc(x:Int) = x+1
MyFunc: (x: Int)Int

scala> val myFunc2 = (x:Int) => x+1
myFunc2: Int => Int = <function1>
(함수가 변수 안에 들어갈 수 있으므로 함수도 나중에 매개변수로 사용가능하다.)

scala> val textFile = sc.textFile("file:///data/hadoop/spark-2.3.0-bin-hadoop2.7/README.md")
textFile: org.apache.spark.rdd.RDD[String] = hdfs:///data/hadoop/spark-2.3.0-bin-hadoop2.7/README.md MapPartitionsRDD[1] at textFile at <console>:24
(에러는 무조건 안남, 실행계획만 있을 뿐 아직 액션이 없었기 때문, 나중에 액션이 일어나면 그때 에러가 난다)

scala> val counts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at <console>:25

scala> counts.collect



[hadoop@s1 jars]$ spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster spark-examples_2.11-2.3.0.jar

