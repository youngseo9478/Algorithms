0404
Hive를 써보자
SQL유사 인터페이스를 이용해서 데이터웨어하우스 인프라 구조로 데이터 요약 질의 분석이 가능합니다.
SQL과 유사하고 메타데이터를 관리합니다.
이것도 맵리듀스로 돌아갑니다.

하이브 설치
[root@s1 ~]#wget 미러주소
[root@s1 ~]# tar -xzf apache-hive-2.3.3-bin.tar.gz
[root@s1 ~]# mv apache-hive-2.3.3-bin /data/hadoop/

[hadoop@s1 ~]$ vi .bashrc

#For Hive settion
export HIVE_HOME=/data/hadoop/apache-hive-2.3.3-bin
export HIVE_CONF=$HIVE_HOME/conf
export PATH=$PATH:$HIVE_HOME/bin

[hadoop@s1 ~]$ source .bashrc

스키마 생성
[hadoop@s1 ~]$ schematool -initSchema -dbType derby

주의! 이걸 하기 전에 하이브에 접속했다면 metastore_db가 생성되어있음
그런데 이건 제대로 작동하지 않는다고 함 이유는 모름
그래서 명령어를 실행시키면

Error: FUNCTION 'NUCLEUS_ASCII' already exists. (state=X0Y68,code=30000)
org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED                                                                             ! Metastore state would be inconsistent !!
Underlying cause: java.io.IOException : Schema script failed, errorcode 2
Use --verbose for detailed stacktrace.
*** schemaTool failed ***

의 에러가 떨어짐. 그렇기 때문에 metastore_db를 rm -r metastore_db로 지우고 다시 명령어를 실행해야 합니다.

하이브에서 !로 시작하면 리눅스 명령어 
첫 명령어는 더비 디비에 초기화 작업 때문에 좀 오래 걸림

피그랑 뭐든 편하던 피그랑 달리
하이브는 스키마는 정의하되 데이터를 읽을 때 스키마를 선택
student table을 정의 하고 타입도 정의하고 해야해
그래서 정의만 해두면 정의한 형태로 들어옵니다.

hive> CREATE TABLE student(name string, age int, score int);
SQL구문에서 예약어들은 대문자로 적는다. 이유는 SQL을 처음에 분석할 때 메모리에서 읽는데 
보통 대문자로 쓰니까 대문자가 메모리에 올라가있다. 그래서 대문자로 구문을 쓰면 메모리 히트 하기 때문에 성능이 쪼금 더 좋아짐
일관성 있게 쓰는 것이 좋다.

hive> load data local inpath '/home/hadoop/student' overwrite into table student;

hive> select * from student
    > ;
OK
hog,10,30       NULL    NULL
kim,30,50       NULL    NULL
park,24,80      NULL    NULL

hive> CREATE TABLE student2(name STRING, age INT, socre INT)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

hive> load data local inpath '/home/hadoop/student' overwrite into table student2;

hive> select * from student2
    > ;
OK
hog     10      30
kim     30      50
park    24      80
Time taken: 0.329 seconds, Fetched: 3 row(s)

hive가 maria_db에 접근가능하게 하고 db를 maria_db를 쓰게 만드는 법

mariadb-java-client-1.5.2.jar 를 일단 받는다.

[hadoop@s1 ~]$ mv mariadb-java-client-1.5.2.jar /data/hadoop/apache-hive-2.3.3-bin/lib/ 로 이동시킨다.
그런데 여기서 
mv: cannot create regular file ‘/data/hadoop/apache-hive-2.3.3-bin/lib/mariadb-java-client-1.5.2.jar’: Permission denied
의 오류가 생기기도 한다.
[hadoop@s1 ~]$ ls -l
로 권한을 보자
-rw-rw-r--. 1 hadoop hadoop 689413344 Apr  3 13:01 2008.csv
-rw-rw-r--. 1 hadoop hadoop    137698 Mar 27 07:47 56850-0.txt
-rw-rw-r--. 1 hadoop hadoop     13943 Apr  3 13:39 airline-0.0.1-SNAPSHOT.jar
-rw-rw-r--. 1 hadoop hadoop  20723870 Apr  4 09:00 apache-hive-2.3.3-bin.tar.gz
-rw-rw-r--. 1 hadoop hadoop   1918152 Apr  4 09:01 apache-hive-2.3.3-bin.tar.gz.1
-rw-rw-r--. 1 hadoop hadoop  42387892 Apr  4 09:06 apache-hive-2.3.3-bin.tar.gz.2
-rw-rw-r--. 1 hadoop hadoop   8935595 Apr  4 09:07 apache-hive-2.3.3-bin.tar.gz.3
-rw-rw-r--. 1 hadoop hadoop     43758 Apr  3 13:01 carriers.csv
-rw-rw-r--. 1 hadoop hadoop        82 Apr  3 08:50 dept.csv
-rw-rw-r--. 1 hadoop hadoop       657 Apr  4 10:03 derby.log
-rw-rw-r--. 1 hadoop hadoop       663 Apr  3 08:50 emp.csv
-rw-rw-r--. 1 hadoop hadoop      7625 Apr  3 10:19 empdept-0.0.1-SNAPSHOT.jar
-rw-rw-r--. 1 hadoop hadoop      5962 Apr  2 14:12 hadoop-0.0.1-SNAPSHOT.jar
-rw-rw-r--. 1 hadoop hadoop        28 Mar 29 19:43 hello.txt
-rw-rw-r--. 1 hadoop hadoop    424672 Apr  4 10:43 mariadb-java-client-1.5.2.jar
drwxrwxr-x. 5 hadoop hadoop      4096 Apr  4 10:03 metastore_db
-rw-rw-r--. 1 hadoop hadoop      2615 Apr  3 14:05 pig_1522733556231.log
-rw-rw-r--. 1 hadoop hadoop     11886 Apr  3 14:10 pig_1522733802188.log
-rw-rw-r--. 1 hadoop hadoop     13083 Apr  3 14:34 pig_1522734156464.log
-rw-rw-r--. 1 hadoop hadoop      2615 Apr  3 14:41 pig_1522735566930.log
drwxr-xr-x. 2 hadoop root        4096 Mar 29 17:52 _setting
-rw-rw-r--. 1 hadoop hadoop        31 Apr  3 14:12 student
-rw-rw-r--. 1 hadoop hadoop     67927 Mar 29 22:20 w.csv

하둡이 유저 그룹도 하둡임

[hadoop@s1 ~]$ ls -l /data/hadoop/apache-hive-2.3.3-bin/
total 60
drwxr-xr-x. 3 root root  4096 Apr  4 09:17 bin
drwxr-xr-x. 2 root root  4096 Apr  4 09:17 binary-package-licenses
drwxr-xr-x. 2 root root  4096 Apr  4 09:18 conf
drwxr-xr-x. 4 root root    32 Apr  4 09:17 examples
drwxr-xr-x. 7 root root    63 Apr  4 09:18 hcatalog
drwxr-xr-x. 2 root root    43 Apr  4 09:18 jdbc
drwxr-xr-x. 4 root root 12288 Apr  4 09:18 lib
-rw-r--r--. 1 root root 20798 Mar  8 06:07 LICENSE
-rw-r--r--. 1 root root   230 Mar 28 04:53 NOTICE
-rw-r--r--. 1 root root   459 Mar 29 08:24 RELEASE_NOTES.txt
drwxr-xr-x. 4 root root    33 Apr  4 09:17 scripts
근데 복사가 될 목적지의 권한이 root로 되어있다.

[hadoop@s1 ~]$ su root 로 접속
[root@s1 hadoop]# chown -R hadoop:hadoop /data/
로 권한을 모두 hadoop으로 바꿔주자.

그 다음 sudo가 가능하게 만드는 법
지금은 root유저만 가능한 상태 이걸 hadoop유저도 쓸 수 있게 변경하자
먼저 su root로 루트유저로 들어가서
[root@s1 hadoop]# visudo
에 들어가서 커맨드 모드에서 :set number 로 라인에 숫자를 붙이자

    101 ## Same thing without a password
    102 # %wheel        ALL=(ALL)       NOPASSWD: ALL
    103  hadoop         ALL=(ALL)       NOPASSWD: ALL
103라인을 추가해주자
:wq

그 다음 su hadoop으로 들어가서
[hadoop@s1 ~]$ sudo systemctl start mariadb
로 마리아 디비를 강제로 스타트 해주자. 앞의 sudo 처리를 안하면 hadoop에선 권한이 없어서 실행이 안된다.

그 다음 마리아디비에 접속 유저를 만들자

[hadoop@s1 ~]$ mysql -uroot -padmin
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 5
Server version: 5.5.56-MariaDB MariaDB Server

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> CREATE DATABASE hive_meta;
Query OK, 1 row affected (0.02 sec)

MariaDB [(none)]> CREATE USER 'hive@%' IDENTIFIED BY 'admin';
Query OK, 0 rows affected (0.13 sec)

MariaDB [(none)]> GRANT ALL on hive_meta.* 'hive'@localhost IDENTIFIED BY 'admin';
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near ''hive'@localhost IDENTIFIED BY 'admin'' at line 1
MariaDB [(none)]> GRANT ALL on hive_meta.* to 'hive'@localhost IDENTIFIED BY 'admin';
Query OK, 0 rows affected (0.02 sec)

MariaDB [(none)]> GRANT ALL on hive_meta.* to 'hive'@'%' IDENTIFIED BY 'admin';
Query OK, 0 rows affected (0.00 sec)

유저와 디비를 마리아비디에서 생성한 후

[hadoop@s1 ~]$ cd /data/hadoop/apache-hive-2.3.3-bin/conf
로 폴더를 이동한다.
[hadoop@s1 conf]$ cp ~/hive-site.xml ./
로 파일을 옮긴다.
[hadoop@s1 conf]$ vi hive-site.xml
를 써서 안에 설정이 혹시 다른게 있으면 변경해준다. 비번을 변경했음

[hadoop@s1 conf]$ schematool -initSchema -dbType mysql
로 스키마를 마리아db에 세팅하자

dept로 실습해봅시당.

hive> CREATE TABLE dept(deptno INT, dname STRING, dloc STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
OK
Time taken: 1.401 seconds


hive> load data inpath '/emp_dept/in/dept.csv' overwrite into table dept;
Loading data to table default.dept
OK
Time taken: 1.366 seconds


hive> select * from dept;
OK
10      ACCOUNTING      NEW YORK
20      RESEARCH        DALLAS
30      SALES   CHICAGO
40      OPERATIONS      BOSTON
Time taken: 5.488 seconds, Fetched: 4 row(s)

그런데 하룹파일서버에 가서 보면 거기에 있던 dept.csv가 사라진걸 볼 수 있다. 
hive> !hdfs dfs -ls /emp_dept/in;
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/hadoop/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/hadoop/hadoop-2.8.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Found 2 items
-rw-r--r--   3 hadoop supergroup        663 2018-04-03 08:58 /emp_dept/in/emp.csv
drwxr-xr-x   - hadoop supergroup          0 2018-04-03 10:23 /emp_dept/in/empdept_out

하이브의 기본 정책은 데이터를 테이블에 넣으면 해당 데이터를 테이블이 관리하는 경로상으로 가져오게 함.

안갖고오고 그냥 참조만하는 방식이 '외부테이블'

외부테이블 실습은 emp.csv로
hive> CREATE EXTERNAL TABLE emp(empno INT,ename STRING, job STRING, mgr INT, hiredate STRING, sal INT, comm INT, deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/emp_dept/in';
CREATE EXTERNAL TABLE 부분과 LOCATION '/emp_dept/in' 부분이 다르다.
이것만으로도 load를 할 필요가 없이 데이터가 들어감
외부테이블은 스키마가 특정 데이터 공간을 참조하는 형식
그냥 테이블은 스키마를 통해 데이터를 불러오는 방식

파일 단위로는 안되기 때문에 대상 폴더 안에 필요한 파일만 모아두는 일이 필요

hive에서 그룹함수 (sum 같은거)를 돌리면 엄청 오래걸림 맵리듀서가 돌아가기 때문에
따라서 그냥 JDBC랑 연동해두고 select정도만 쓰는 것이 좋다.

hive 괴롭히기 - 조인을 걸어보자
SELECT d.deptno, COUNT(*), SUM(e.sal), d.dname, d.dloc
 FROM emp e JOIN dept d ON (e.deptno = d.deptno)
 WHERE e.sal > 1000
 GROUP BY d.deptno, d.dname, d.dloc
 HAVING(SUM(e.sal)>8500);

하이브를 원격에서 접속해보자

우선
하이브서버 백그라운드에서 돌리기
[hadoop@s1 ~]$ hiveserver2 &
[1] 13592
나중에 죽일 땐
kill -9 13592

확인
[hadoop@s1 ~]$ jps
20001 SecondaryNameNode
13893 Jps
19702 NameNode
20278 NodeManager
13592 RunJar
56457 JobHistoryServer
19836 DataNode
20157 ResourceManager

**RunJar가 있음

그 담에 beeline
[hadoop@s1 ~]$ beeline
beeline> !connect jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000: hive
Enter password for jdbc:hive2://localhost:10000: *****


0: jdbc:hive2://localhost:10000> select * from dept;
OK
+--------------+-------------+------------+
| dept.deptno  | dept.dname  | dept.dloc  |
+--------------+-------------+------------+
| 10           | ACCOUNTING  | NEW YORK   |
| 20           | RESEARCH    | DALLAS     |
| 30           | SALES       | CHICAGO    |
| 40           | OPERATIONS  | BOSTON     |
+--------------+-------------+------------+
4 rows selected (3.671 seconds)


이클립스에서 JDBC 연동으로 hive 랑 연동하는거 해봅시다.
메이븐 프로젝트에서 pom.xml에 의존성 설정을 합시다.
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <version>2.3.2</version>
</dependency>


파이썬은 스크립트 언어로 쉬워용
객체지향에서 뻗어나온 스크립트류 함수
인터프리터 방식은 프로그래밍 언어의 문법에 맞게 명령형을 해석한다. 
프로그래밍 언어 문법에 맞춰 코드를 한 줄씩 해석 그래서 에러가 있는 문장이 해석되기 전까지 돌아간다.

Python으로 직접 만들었거나 또는 다른 프로그램의 Wrapper가 꼭 존재한다. 그래서 다양한 언어를 포괄하는게 가능.
리눅스 서버는 파이썬 언어가 조금씩 내장되어있다.

약점 

버전은 2도 있고 3도 있고
라이브러리들이 참 많은데 문제는 2, 3이 연동이 안된다.

파이썬에서도 쓰레드는 쓸 수 있는데 GIL(글로벌 인터프리터 락) 때문에 병렬처리가 안되고 쓰레드만 많이 만들어지지 실제로는 
인터프리터가 하나씩만 실행하기 때문에 결국 쓰레드 중에 하나만 돌아간다.
빡센 작업이거나 2개 이상 돌리고싶으면 파이썬에서 C 모듈을 짜서 붙이거나 처음부터 C를 쓰는게 낫다.


Hive랑 이클립스를 연동해보자

의존성 추가
<dependency>
	<groupId>org.apache.hive</groupId>
	<artifactId>hive-jdbc</artifactId>
	<version>2.3.2</version>
</dependency>

구현
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;


public class HiveJdbcTest {
	public static void main(String[] args) {
		//1. JDBC 연결에 필요한 정보를 준비 url,DBname, username, password
		String JDBC_DRIVER = "org.apache.hive.jdbc.HiveDriver";
		String url = "jdbc:hive2://192.168.111.201:10000";
		String username = "";
		String password = "";
		
		//2. connection객체를 빌드(드라이버 적재, 커넥션 빌드)
		Connection conn = null;
		try {
			Class.forName(JDBC_DRIVER);
			conn = DriverManager.getConnection(url,username,password);
		} catch (Exception e) {
			// TODO: handle exception
		}
		
		//3. 날리고자 하는 SQL문을 Statement 객체로 준비
		String sql = "SELECT * FROM dept";
		PreparedStatement ps = null;
		try {
			ps = conn.prepareStatement(sql);
		} catch (SQLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		//4. 결과값을 회수하고자 한다면 ResultSet 객체 핸들링
		ResultSet rs =null;
		try {
			rs = ps.executeQuery();
			while(rs.next()) {
			System.out.print(rs.getInt("deptno")+"\t");
			System.out.print(rs.getString("dname")+"\t");
			System.out.println(rs.getString("dloc"));
			
			}
		} catch (SQLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}
}
사실상 JDBC구현과 같으며 중간에 url에 하이브 서버와 driver에 하이브 드라이버가 들어간다는 점이 다르다.
그리고 Mybatis를 구현할 때도 똑같이 applicationContext에서 주소와 드라이버만 hive로 설정해두면 된다.
hive에 좋은 점은 sql구문을 알아먹는다는 점이다.
