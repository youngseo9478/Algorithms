0417

스파크 시작
spark-shell


스칼라에서 리스트에 요소 접근 할 때 소괄호로 합니다.(Array의 경우도 동일)
scala> val data = List("1","2","3","4")
data: List[String] = List(1, 2, 3, 4)

scala> data(0)
res0: String = 1

튜플 접근은 ._1로 가능
scala> val tuple = (1,2,3,4)
tuple: (Int, Int, Int, Int) = (1,2,3,4)

scala> tuple.1
<console>:1: error: ';' expected but double literal found.
tuple.1
     ^

scala> tuple._1
res1: Int = 1

스트링을 인트로 전환
scala> val text = "3000"
text: String = 3000

scala> text.toInt
res2: Int = 3000

csv 파일 읽어오기
scala> val emp = sc.textFile("file:///home/hadoop/emp.csv")
emp: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/emp.csv MapPartitionsRDD[1] at textFile at <console>:24

collect함수는 함부로 쓰면 안됨 로컬로 가져오는 것이라 분산해둔 데이터가 로컬로 모여버림

scala> emp.first
res4: String = 7839,KING,PRESIDENT,NULL,1981-11-17,5000,NULL,10


scala> emp.take(5)
res6: Array[String] = Array(7839,KING,PRESIDENT,NULL,1981-11-17,5000,NULL,10, 7698,BLAKE,MANAGER,7839,1981-05-01,2850,NULL,30, 7782,CLARK,MANAGER,7839,1981-05-09,2450,NULL,10, 7566,JONES,MANAGER,7839,1981-04-01,2975,NULL,20, 7654,MARTIN,SALESMAN,7698,1981-09-10,1250,1400,30)

scala> val empRdd = emp.map( line => line.split(","))
scala> val empRdd = emp.map( _.split(","))     (둘다 같은 녀석)

scala> empRdd.first
res7: Array[String] = Array(7839, KING, PRESIDENT, NULL, 1981-11-17, 5000, NULL, 10)

scala> val empRddfilter = empRdd.filter( data => data(5).toInt >1000)
empRddfilter: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at filter at <console>:27

scala> empRddfilter.count
res8: Long = 12

scala> val empRdd3 = empRddfilter.map( emp => (emp(7),emp(5).toInt))
empRdd3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:27

scala> empRdd3.collect
res13: Array[(String, Int)] = Array((10,5000), (30,2850), (10,2450), (20,2975), (30,1250), (30,1600), (30,1500), (30,1250), (20,3000), (20,3000), (20,1100), (10,1300))

scala> val empRdd4 = empRdd3.reduceByKey( _ + _ )
empRdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:25

scala> empRdd4.collect
res14: Array[(String, Int)] = Array((20,10075), (30,8450), (10,8750))

scala> val deptRdd = sc.textFile("file:///home/hadoop/dept.csv")
deptRdd: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/dept.csv MapPartitionsRDD[7] at textFile at <console>:24

scala> val deptRdd2 = deptRdd.map( _.split(","))
deptRdd2: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[8] at map at <console>:25

scala> val deptRdd3 = deptRdd2.map ( dept => (dept(0), dept(1)+","+dept(2)))
deptRdd3: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at map at <console>:25

scala> val result1 = deptRdd3.join(empRdd4)
result1: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[14] at join at <console>:27
스칼라에서 조인을 걸기 위해선 둘 다 map 구조(튜플)로 이루어져있어야 한다 (key,value)

scala> result1.take(3)
res15: Array[(String, (String, Int))] = Array((20,(RESEARCH,DALLAS,10075)), (30,(SALES,CHICAGO,8450)), (10,(ACCOUNTING,NEW YORK,8750)))

val result2 = result1.mapValues( v => v._1 + "," +v._2)

val result3 = result2.map { case (a,b) => Array(a,b).mkString(",") }

result3.saveAsTextFile("file:///home/hadoop/result")


http://192.168.111.201:4040/jobs/ 로 들어가면 내가 했던 작업을 확인할 수 있습니다.


내가 실수로 scala를 나가게 되면 작업했던 내용이 모두 사라집니다. 그럴 때는 [hadoop@s1 ~]$ cat .scala_history
로 들어가서 필요없는 코드를 지우고 전부 다시 실행해봅시다.



이클립스에서 해봅시다
scala IDE 깔고 scala 프로젝트 만들고
Properties에 들어가서 scala compiler를 버전을 맞춰줍시다.
그담에 메이븐 프로젝트로 컨버트 해서 의존성주입을 합시다.

pom.xml에서 
<plugin>
	<groupId>org.scala-tools</groupId>
	<artifactId>maven-scala-plugin</artifactId>
	<executions>
		<execution>
			<id>compile</id>
			<goals>
				<goal>compile</goal>
			</goals>
			<phase>compile</phase>
		</execution>
		<execution>
			<phase>process-resources</phase>
			<goals>
				<goal>compile</goal>
			</goals>
		</execution>
	</executions>
</plugin>

을 <plugins> 아래에 넣어줍니다. 그러면 <execution>에서 에러가 날 것입니다.
그때 <pluginManagment>와 <pluginRepositories><pluginRepository>를 추가해줘야합니다.
레파지토리는 빌드 밖에 매니지먼트는 빌드 안에 넣어줍니다


그 다음에 레파지토리를 추가해서 빌딩해줍니다.
<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>2.3.0</version>
</dependency>


그다음에 

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

object EmpDept {
  def main(args: Array[String]): Unit = {
    val sc = new SparkContext(new SparkConf().setAppName("empdept"))

    val emp = sc.textFile(args(0))
    val empRdd = emp.map(line => line.split(","))
    val empRddfilter = empRdd.filter(data => data(5).toInt > 1000)
    val empRdd3 = empRddfilter.map(emp => (emp(7), emp(5).toInt))
    val empRdd4 = empRdd3.reduceByKey(_ + _)
    val deptRdd = sc.textFile(args(1))
    val deptRdd2 = deptRdd.map(_.split(","))
    val deptRdd3 = deptRdd2.map(dept => (dept(0), dept(1) + "," + dept(2)))
    val result1 = deptRdd3.join(empRdd4)
    val result2 = result1.mapValues(v => v._1 + "," + v._2)
    val result3 = result2.map { case (a, b) => Array(a, b).mkString(",") }
    result3.saveAsTextFile(args(2))
  }
}

필요한 대상 파일들은 hdfs 안에 넣어두고

[hadoop@s1 ~]$ spark-submit --master yarn --deploy-mode cluster --class scala.EmpDept scala-0.0.1-SNAPSHOT.jar /test_emp/emp.csv /test_emp/dept.csv /test_emp/out2
